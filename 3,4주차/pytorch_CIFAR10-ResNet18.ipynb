{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOzAFUPRZzVuCtSRpoulfV3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":17,"metadata":{"id":"HF-XFpoe7tvm","executionInfo":{"status":"ok","timestamp":1688724116791,"user_tz":-540,"elapsed":1141,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"outputs":[],"source":["# 모델 Resnet 18 적용\n","\n","# 이제 모델과 데이터가 준비되었으니, 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트할 차례입니다.\n","# 모델을 학습하는 과정은 반복적인 과정을 거칩니다; 각 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류(손실(loss))를 계산하고,\n","# (이전 장에서 본 것처럼) 매개변수에 대한 오류의 도함수(derivative)를 수집한 뒤,\n","# 경사하강법을 사용하여 이 파라미터들을 최적화(optimize)합니다.\n","\n","import torch\n","from torch import nn\n","\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","import torch.backends.cudnn as cudnn\n"]},{"cell_type":"code","source":["#stride = 건너뛰는 간격. 즉, 여기선 데이터를 얼마나 건너뛰며 볼 것임을 설정. ex : 1= 1픽셀당 하나씩 본다. 2= 2픽셀당 하나씩 본다. (출력 이미지 1/4로 줄어듬.) 3= 3픽셀당 하나씩. (출력 이미지 1/9로 축소)\n","#kernel = 특징 추출해낼 커널 (필터). 논문에선 7*7사이즈.\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias = False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","\n","        # 만약 출력 이미지가 입력보다 축소 됐을 시, shortcut 덧셈도 축소된 이미지에 맞춰 행렬 크기를 조정해야 하므로, 그 코드임 아래는\n","        if stride !=1:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes, kernel_size=1, stride = stride, bias=False),\n","                nn.BatchNorm2d(planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out"],"metadata":{"id":"yISkXUa_7Ji5","executionInfo":{"status":"ok","timestamp":1688724116791,"user_tz":-540,"elapsed":4,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#resnet 18 정의\n","\n","class Resnet(nn.Module):\n","    def __init__(self, block, num_block, num_classes=10):\n","        super(Resnet, self).__init__()\n","        self.in_planes=64\n","\n","        self.conv1 = nn.Conv2d(3,64,kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block,64, num_block[0], stride=1)\n","        self.layer2 = self._make_layer(block,128, num_block[1], stride=2)\n","        self.layer3 = self._make_layer(block,256, num_block[2], stride=2)\n","        self.layer4 = self._make_layer(block,512, num_block[3], stride=2)\n","        self.linear = nn.Linear(512,num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self,x):\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x) # 4x4x512\n","        x = F.avg_pool2d(x,4) #1x1x512\n","        x = x.view(x.size(0), -1)\n","        x = self.linear(x)\n","        return x\n","\n","def Resnet18():\n","    return Resnet(BasicBlock,[2,2,2,2])"],"metadata":{"id":"R5lQkdKH_nTu","executionInfo":{"status":"ok","timestamp":1688724116792,"user_tz":-540,"elapsed":5,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# CIFAR-10 데이터 준비\n","\n","import torchvision\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Normalize the test set same as training set without augmentation\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","training_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=transform_train\n",")\n","\n","test_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=transform_test\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True, num_workers=4)\n","test_dataloader = DataLoader(test_data, batch_size=32, shuffle =False, num_workers=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55h-I_KS5gYV","executionInfo":{"status":"ok","timestamp":1688724119059,"user_tz":-540,"elapsed":2271,"user":{"displayName":"바다","userId":"15013850930390190862"}},"outputId":"3597244f-d3ac-4ade-fa73-bce88b5f7f93"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# 하이퍼파라미터(Hyperparameter)\n","\n","# 하이퍼파라미터(Hyperparameter)는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수입니다.\n","# 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있습니다.\n","\n","# 학습 시에는 다음과 같은 하이퍼파라미터를 정의합니다:\n","# 에폭(epoch) 수 - 데이터셋을 반복하는 횟수\n","\n","# 배치 크기(batch size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n","\n","# 학습률(learning rate) - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다.\n"],"metadata":{"id":"y__U0zlp8wJH","executionInfo":{"status":"ok","timestamp":1688724119059,"user_tz":-540,"elapsed":7,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 최적화 단계(Optimization Loop)\n","\n","# 하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있습니다. 최적화 단계의 각 반복(iteration)을 에폭이라고 부릅니다.\n","\n","# 하나의 에폭은 다음 두 부분으로 구성됩니다:\n","# 학습 단계(train loop) - 학습용 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴합니다.\n","\n","# 검증/테스트 단계(validation/test loop) - 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)합니다."],"metadata":{"id":"CjVmyF2R-Gcx","executionInfo":{"status":"ok","timestamp":1688724119059,"user_tz":-540,"elapsed":6,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 손실 함수(loss function)\n","\n","# 학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높습니다.\n","# 손실 함수(loss function)는 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정하며, 학습 중에 이 값을 최소화하려고 합니다.\n","#  주어진 데이터 샘플을 입력으로 계산한 예측과 정답(label)을 비교하여 손실(loss)을 계산합니다.\n","\n","# 손실 함수 초기화\n","\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"jViXhTu_-Ouq","executionInfo":{"status":"ok","timestamp":1688724119059,"user_tz":-540,"elapsed":5,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# 옵티마이저(Optimizer)\n","\n","# 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다.\n","# 최적화 알고리즘은 이 과정이 수행되는 방식(여기에서는 확률적 경사하강법(SGD; Stochastic Gradient Descent))을 정의합니다\n","\n","# 학습하려는 모델의 매개변수와 학습률(learning rate) 하이퍼파라미터를 등록하여 옵티마이저를 초기화합니다.\n","\n","\n","\n"],"metadata":{"id":"V0T3NCuY-huY","executionInfo":{"status":"ok","timestamp":1688724119060,"user_tz":-540,"elapsed":6,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 학습 단계(loop)에서 최적화는 세단계로 이뤄집니다:\n","# optimizer.zero_grad()를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정합니다.\n","\n","# loss.backwards()를 호출하여 예측 손실(prediction loss)을 역전파합니다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장합니다.\n","\n","# 변화도를 계산한 뒤에는 optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정합니다."],"metadata":{"id":"9wl_Z-nU-0wR","executionInfo":{"status":"ok","timestamp":1688724119060,"user_tz":-540,"elapsed":6,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# training 메소드 정의\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","net = Resnet18()\n","net = net.to(device)\n","cudnn.benchmark=True\n","\n","\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay = 0.0002)\n","\n","def train(epoch):\n","    print(f'\\n[Train epoch: {epoch}]')\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = loss_fn(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        if batch_idx % 100 == 99:\n","            print('\\nCurrent batch:', str(batch_idx+1))\n","            print('Current train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n","            print('Current train loss:', loss.item())\n","\n","    print('\\nTotal train accuarcy:', 100. * correct / total)\n","    print('Total train loss:', train_loss)"],"metadata":{"id":"m-hgoRsVEP-t","executionInfo":{"status":"ok","timestamp":1688724119060,"user_tz":-540,"elapsed":5,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# test 메소드 정의\n","\n","\n","def test(epoch):\n","    print(f'\\n[ Test epoch: {epoch} ]')\n","    net.eval()\n","    loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(test_dataloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = net(inputs)\n","        loss += loss_fn(outputs, targets).item()\n","        total += targets.size(0)\n","\n","        _,predicted = outputs.max(1)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    print('\\nTest accuarcy:', 100. * correct / total)\n","    print('Test average loss:', loss / total)\n","\n","    if not os.path.isdir('checkpoint'):\n","        os.mkdir('checkpoint')\n","    torch.save(net, './checkpoint/resnet18_cifar10_model.pt')\n","    torch.save(net.state_dict(), './checkpoint/resnet18_cifar10_state_dict.pt')\n","    torch.save({\n","        'model' : net.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }, './checkpoint/resnet18_cifar10_all.tar')\n","    print('Model Saved!')"],"metadata":{"id":"uPKH-yPY-7oo","executionInfo":{"status":"ok","timestamp":1688724119060,"user_tz":-540,"elapsed":5,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#실제 동작부\n","for epoch in range(0,20):\n","\n","    train(epoch)\n","    test(epoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-sPM1MJFY8A","executionInfo":{"status":"ok","timestamp":1688725276364,"user_tz":-540,"elapsed":1157309,"user":{"displayName":"바다","userId":"15013850930390190862"}},"outputId":"c2ca9118-5ae8-4abd-aba0-ece0471cd301"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[Train epoch: 0]\n","\n","Current batch: 100\n","Current train accuracy: 0.125\n","Current train loss: 2.2302143573760986\n","\n","Current batch: 200\n","Current train accuracy: 0.28125\n","Current train loss: 1.9517881870269775\n","\n","Current batch: 300\n","Current train accuracy: 0.28125\n","Current train loss: 1.824252724647522\n","\n","Current batch: 400\n","Current train accuracy: 0.28125\n","Current train loss: 2.030756711959839\n","\n","Current batch: 500\n","Current train accuracy: 0.3125\n","Current train loss: 1.917461633682251\n","\n","Current batch: 600\n","Current train accuracy: 0.3125\n","Current train loss: 1.8730064630508423\n","\n","Current batch: 700\n","Current train accuracy: 0.3125\n","Current train loss: 1.8083007335662842\n","\n","Current batch: 800\n","Current train accuracy: 0.375\n","Current train loss: 1.665457010269165\n","\n","Current batch: 900\n","Current train accuracy: 0.375\n","Current train loss: 1.6763629913330078\n","\n","Current batch: 1000\n","Current train accuracy: 0.25\n","Current train loss: 1.8090587854385376\n","\n","Current batch: 1100\n","Current train accuracy: 0.40625\n","Current train loss: 1.7653687000274658\n","\n","Current batch: 1200\n","Current train accuracy: 0.46875\n","Current train loss: 1.7256743907928467\n","\n","Current batch: 1300\n","Current train accuracy: 0.46875\n","Current train loss: 1.4429163932800293\n","\n","Current batch: 1400\n","Current train accuracy: 0.21875\n","Current train loss: 1.8041075468063354\n","\n","Current batch: 1500\n","Current train accuracy: 0.28125\n","Current train loss: 1.5354576110839844\n","\n","Total train accuarcy: 30.682\n","Total train loss: 2930.904087781906\n","\n","[ Test epoch: 0 ]\n","\n","Test accuarcy: 37.31\n","Test average loss: 0.05525631417036057\n","Model Saved!\n","\n","[Train epoch: 1]\n","\n","Current batch: 100\n","Current train accuracy: 0.28125\n","Current train loss: 1.8775800466537476\n","\n","Current batch: 200\n","Current train accuracy: 0.375\n","Current train loss: 1.4336668252944946\n","\n","Current batch: 300\n","Current train accuracy: 0.5\n","Current train loss: 1.534607172012329\n","\n","Current batch: 400\n","Current train accuracy: 0.53125\n","Current train loss: 1.5967817306518555\n","\n","Current batch: 500\n","Current train accuracy: 0.625\n","Current train loss: 1.2240570783615112\n","\n","Current batch: 600\n","Current train accuracy: 0.46875\n","Current train loss: 1.4356963634490967\n","\n","Current batch: 700\n","Current train accuracy: 0.40625\n","Current train loss: 1.5167226791381836\n","\n","Current batch: 800\n","Current train accuracy: 0.625\n","Current train loss: 0.9175321459770203\n","\n","Current batch: 900\n","Current train accuracy: 0.3125\n","Current train loss: 1.7245140075683594\n","\n","Current batch: 1000\n","Current train accuracy: 0.46875\n","Current train loss: 1.7149783372879028\n","\n","Current batch: 1100\n","Current train accuracy: 0.625\n","Current train loss: 1.4322510957717896\n","\n","Current batch: 1200\n","Current train accuracy: 0.4375\n","Current train loss: 1.5095425844192505\n","\n","Current batch: 1300\n","Current train accuracy: 0.4375\n","Current train loss: 1.0970076322555542\n","\n","Current batch: 1400\n","Current train accuracy: 0.5\n","Current train loss: 1.3471966981887817\n","\n","Current batch: 1500\n","Current train accuracy: 0.5\n","Current train loss: 1.3076512813568115\n","\n","Total train accuarcy: 49.866\n","Total train loss: 2161.105764389038\n","\n","[ Test epoch: 1 ]\n","\n","Test accuarcy: 56.76\n","Test average loss: 0.03730969687700272\n","Model Saved!\n","\n","[Train epoch: 2]\n","\n","Current batch: 100\n","Current train accuracy: 0.59375\n","Current train loss: 1.0669606924057007\n","\n","Current batch: 200\n","Current train accuracy: 0.71875\n","Current train loss: 1.0407942533493042\n","\n","Current batch: 300\n","Current train accuracy: 0.5\n","Current train loss: 1.434141755104065\n","\n","Current batch: 400\n","Current train accuracy: 0.5625\n","Current train loss: 1.0714609622955322\n","\n","Current batch: 500\n","Current train accuracy: 0.71875\n","Current train loss: 1.0149328708648682\n","\n","Current batch: 600\n","Current train accuracy: 0.6875\n","Current train loss: 1.1238833665847778\n","\n","Current batch: 700\n","Current train accuracy: 0.6875\n","Current train loss: 0.8875202536582947\n","\n","Current batch: 800\n","Current train accuracy: 0.59375\n","Current train loss: 1.1398779153823853\n","\n","Current batch: 900\n","Current train accuracy: 0.5\n","Current train loss: 1.2491835355758667\n","\n","Current batch: 1000\n","Current train accuracy: 0.5\n","Current train loss: 1.1489382982254028\n","\n","Current batch: 1100\n","Current train accuracy: 0.59375\n","Current train loss: 1.0257457494735718\n","\n","Current batch: 1200\n","Current train accuracy: 0.78125\n","Current train loss: 0.7609269022941589\n","\n","Current batch: 1300\n","Current train accuracy: 0.5625\n","Current train loss: 0.91581130027771\n","\n","Current batch: 1400\n","Current train accuracy: 0.65625\n","Current train loss: 0.7993770241737366\n","\n","Current batch: 1500\n","Current train accuracy: 0.6875\n","Current train loss: 0.7915136814117432\n","\n","Total train accuarcy: 62.746\n","Total train loss: 1641.8993523418903\n","\n","[ Test epoch: 2 ]\n","\n","Test accuarcy: 66.66\n","Test average loss: 0.030578448525071144\n","Model Saved!\n","\n","[Train epoch: 3]\n","\n","Current batch: 100\n","Current train accuracy: 0.59375\n","Current train loss: 1.0556235313415527\n","\n","Current batch: 200\n","Current train accuracy: 0.71875\n","Current train loss: 0.8446522355079651\n","\n","Current batch: 300\n","Current train accuracy: 0.71875\n","Current train loss: 0.7897835969924927\n","\n","Current batch: 400\n","Current train accuracy: 0.59375\n","Current train loss: 1.040077567100525\n","\n","Current batch: 500\n","Current train accuracy: 0.78125\n","Current train loss: 0.6950870156288147\n","\n","Current batch: 600\n","Current train accuracy: 0.71875\n","Current train loss: 1.1267281770706177\n","\n","Current batch: 700\n","Current train accuracy: 0.78125\n","Current train loss: 0.7400174736976624\n","\n","Current batch: 800\n","Current train accuracy: 0.8125\n","Current train loss: 0.6145073771476746\n","\n","Current batch: 900\n","Current train accuracy: 0.71875\n","Current train loss: 0.8679057359695435\n","\n","Current batch: 1000\n","Current train accuracy: 0.71875\n","Current train loss: 0.7891921401023865\n","\n","Current batch: 1100\n","Current train accuracy: 0.6875\n","Current train loss: 0.8689541816711426\n","\n","Current batch: 1200\n","Current train accuracy: 0.6875\n","Current train loss: 1.0233867168426514\n","\n","Current batch: 1300\n","Current train accuracy: 0.8125\n","Current train loss: 0.5710628032684326\n","\n","Current batch: 1400\n","Current train accuracy: 0.6875\n","Current train loss: 0.987056314945221\n","\n","Current batch: 1500\n","Current train accuracy: 0.59375\n","Current train loss: 0.9367149472236633\n","\n","Total train accuarcy: 70.3\n","Total train loss: 1330.605893790722\n","\n","[ Test epoch: 3 ]\n","\n","Test accuarcy: 71.71\n","Test average loss: 0.02636595884859562\n","Model Saved!\n","\n","[Train epoch: 4]\n","\n","Current batch: 100\n","Current train accuracy: 0.75\n","Current train loss: 0.7244492769241333\n","\n","Current batch: 200\n","Current train accuracy: 0.78125\n","Current train loss: 0.7788096070289612\n","\n","Current batch: 300\n","Current train accuracy: 0.71875\n","Current train loss: 0.7595456838607788\n","\n","Current batch: 400\n","Current train accuracy: 0.6875\n","Current train loss: 0.9223991632461548\n","\n","Current batch: 500\n","Current train accuracy: 0.59375\n","Current train loss: 1.0171531438827515\n","\n","Current batch: 600\n","Current train accuracy: 0.78125\n","Current train loss: 0.4894959628582001\n","\n","Current batch: 700\n","Current train accuracy: 0.6875\n","Current train loss: 0.9977370500564575\n","\n","Current batch: 800\n","Current train accuracy: 0.84375\n","Current train loss: 0.6227712035179138\n","\n","Current batch: 900\n","Current train accuracy: 0.75\n","Current train loss: 0.9059556126594543\n","\n","Current batch: 1000\n","Current train accuracy: 0.8125\n","Current train loss: 0.5886102914810181\n","\n","Current batch: 1100\n","Current train accuracy: 0.8125\n","Current train loss: 0.6140463948249817\n","\n","Current batch: 1200\n","Current train accuracy: 0.59375\n","Current train loss: 1.1290080547332764\n","\n","Current batch: 1300\n","Current train accuracy: 0.71875\n","Current train loss: 0.9587873220443726\n","\n","Current batch: 1400\n","Current train accuracy: 0.84375\n","Current train loss: 0.46553003787994385\n","\n","Current batch: 1500\n","Current train accuracy: 0.78125\n","Current train loss: 0.5934046506881714\n","\n","Total train accuarcy: 74.046\n","Total train loss: 1165.062001556158\n","\n","[ Test epoch: 4 ]\n","\n","Test accuarcy: 78.74\n","Test average loss: 0.0195118296161294\n","Model Saved!\n","\n","[Train epoch: 5]\n","\n","Current batch: 100\n","Current train accuracy: 0.65625\n","Current train loss: 0.8715357184410095\n","\n","Current batch: 200\n","Current train accuracy: 0.71875\n","Current train loss: 0.69059818983078\n","\n","Current batch: 300\n","Current train accuracy: 0.78125\n","Current train loss: 0.7568146586418152\n","\n","Current batch: 400\n","Current train accuracy: 0.75\n","Current train loss: 0.8111106753349304\n","\n","Current batch: 500\n","Current train accuracy: 0.71875\n","Current train loss: 0.6183838248252869\n","\n","Current batch: 600\n","Current train accuracy: 0.84375\n","Current train loss: 0.46878597140312195\n","\n","Current batch: 700\n","Current train accuracy: 0.59375\n","Current train loss: 0.8734790086746216\n","\n","Current batch: 800\n","Current train accuracy: 0.8125\n","Current train loss: 0.5880627632141113\n","\n","Current batch: 900\n","Current train accuracy: 0.75\n","Current train loss: 0.7160471677780151\n","\n","Current batch: 1000\n","Current train accuracy: 0.71875\n","Current train loss: 0.8415790796279907\n","\n","Current batch: 1100\n","Current train accuracy: 0.8125\n","Current train loss: 0.5898598432540894\n","\n","Current batch: 1200\n","Current train accuracy: 0.6875\n","Current train loss: 0.7042940258979797\n","\n","Current batch: 1300\n","Current train accuracy: 0.84375\n","Current train loss: 0.45183810591697693\n","\n","Current batch: 1400\n","Current train accuracy: 0.84375\n","Current train loss: 0.5165026783943176\n","\n","Current batch: 1500\n","Current train accuracy: 0.71875\n","Current train loss: 0.8347707390785217\n","\n","Total train accuarcy: 76.958\n","Total train loss: 1048.470112502575\n","\n","[ Test epoch: 5 ]\n","\n","Test accuarcy: 76.15\n","Test average loss: 0.02143355360329151\n","Model Saved!\n","\n","[Train epoch: 6]\n","\n","Current batch: 100\n","Current train accuracy: 0.78125\n","Current train loss: 0.5861849784851074\n","\n","Current batch: 200\n","Current train accuracy: 0.8125\n","Current train loss: 0.4486939311027527\n","\n","Current batch: 300\n","Current train accuracy: 0.71875\n","Current train loss: 0.6333301663398743\n","\n","Current batch: 400\n","Current train accuracy: 0.71875\n","Current train loss: 0.9401808977127075\n","\n","Current batch: 500\n","Current train accuracy: 0.5625\n","Current train loss: 1.1260515451431274\n","\n","Current batch: 600\n","Current train accuracy: 0.625\n","Current train loss: 1.0194011926651\n","\n","Current batch: 700\n","Current train accuracy: 0.8125\n","Current train loss: 0.5965337157249451\n","\n","Current batch: 800\n","Current train accuracy: 0.78125\n","Current train loss: 0.6417713165283203\n","\n","Current batch: 900\n","Current train accuracy: 0.71875\n","Current train loss: 0.7495536804199219\n","\n","Current batch: 1000\n","Current train accuracy: 0.75\n","Current train loss: 1.078192949295044\n","\n","Current batch: 1100\n","Current train accuracy: 0.9375\n","Current train loss: 0.3976191282272339\n","\n","Current batch: 1200\n","Current train accuracy: 0.75\n","Current train loss: 0.8109080791473389\n","\n","Current batch: 1300\n","Current train accuracy: 0.90625\n","Current train loss: 0.4672301411628723\n","\n","Current batch: 1400\n","Current train accuracy: 0.75\n","Current train loss: 0.695899248123169\n","\n","Current batch: 1500\n","Current train accuracy: 0.8125\n","Current train loss: 0.6516686081886292\n","\n","Total train accuarcy: 78.404\n","Total train loss: 981.5949010998011\n","\n","[ Test epoch: 6 ]\n","\n","Test accuarcy: 76.07\n","Test average loss: 0.02247182051241398\n","Model Saved!\n","\n","[Train epoch: 7]\n","\n","Current batch: 100\n","Current train accuracy: 0.9375\n","Current train loss: 0.30484381318092346\n","\n","Current batch: 200\n","Current train accuracy: 0.8125\n","Current train loss: 0.5089386701583862\n","\n","Current batch: 300\n","Current train accuracy: 0.78125\n","Current train loss: 0.6055484414100647\n","\n","Current batch: 400\n","Current train accuracy: 0.65625\n","Current train loss: 1.096603512763977\n","\n","Current batch: 500\n","Current train accuracy: 0.71875\n","Current train loss: 0.7164694666862488\n","\n","Current batch: 600\n","Current train accuracy: 0.78125\n","Current train loss: 0.7776656746864319\n","\n","Current batch: 700\n","Current train accuracy: 0.71875\n","Current train loss: 0.7335079908370972\n","\n","Current batch: 800\n","Current train accuracy: 0.84375\n","Current train loss: 0.5776374936103821\n","\n","Current batch: 900\n","Current train accuracy: 0.71875\n","Current train loss: 0.5864672660827637\n","\n","Current batch: 1000\n","Current train accuracy: 0.75\n","Current train loss: 0.6881213784217834\n","\n","Current batch: 1100\n","Current train accuracy: 0.90625\n","Current train loss: 0.395220011472702\n","\n","Current batch: 1200\n","Current train accuracy: 0.84375\n","Current train loss: 0.4530944228172302\n","\n","Current batch: 1300\n","Current train accuracy: 0.875\n","Current train loss: 0.3392523527145386\n","\n","Current batch: 1400\n","Current train accuracy: 0.78125\n","Current train loss: 0.8466989994049072\n","\n","Current batch: 1500\n","Current train accuracy: 0.78125\n","Current train loss: 0.6102075576782227\n","\n","Total train accuarcy: 79.49\n","Total train loss: 935.6882300972939\n","\n","[ Test epoch: 7 ]\n","\n","Test accuarcy: 79.23\n","Test average loss: 0.01833887578845024\n","Model Saved!\n","\n","[Train epoch: 8]\n","\n","Current batch: 100\n","Current train accuracy: 0.78125\n","Current train loss: 0.6308174133300781\n","\n","Current batch: 200\n","Current train accuracy: 0.625\n","Current train loss: 1.046818733215332\n","\n","Current batch: 300\n","Current train accuracy: 0.78125\n","Current train loss: 0.4432763457298279\n","\n","Current batch: 400\n","Current train accuracy: 0.78125\n","Current train loss: 0.696718156337738\n","\n","Current batch: 500\n","Current train accuracy: 0.84375\n","Current train loss: 0.4787750840187073\n","\n","Current batch: 600\n","Current train accuracy: 0.84375\n","Current train loss: 0.3674844205379486\n","\n","Current batch: 700\n","Current train accuracy: 0.8125\n","Current train loss: 0.43466266989707947\n","\n","Current batch: 800\n","Current train accuracy: 0.78125\n","Current train loss: 0.5800378918647766\n","\n","Current batch: 900\n","Current train accuracy: 0.75\n","Current train loss: 0.6619095206260681\n","\n","Current batch: 1000\n","Current train accuracy: 0.84375\n","Current train loss: 0.5170271396636963\n","\n","Current batch: 1100\n","Current train accuracy: 0.71875\n","Current train loss: 0.6626299619674683\n","\n","Current batch: 1200\n","Current train accuracy: 0.8125\n","Current train loss: 0.6133557558059692\n","\n","Current batch: 1300\n","Current train accuracy: 0.9375\n","Current train loss: 0.4510408341884613\n","\n","Current batch: 1400\n","Current train accuracy: 0.84375\n","Current train loss: 0.38444459438323975\n","\n","Current batch: 1500\n","Current train accuracy: 0.78125\n","Current train loss: 0.631306529045105\n","\n","Total train accuarcy: 80.202\n","Total train loss: 900.0939533859491\n","\n","[ Test epoch: 8 ]\n","\n","Test accuarcy: 80.41\n","Test average loss: 0.01788247580975294\n","Model Saved!\n","\n","[Train epoch: 9]\n","\n","Current batch: 100\n","Current train accuracy: 0.8125\n","Current train loss: 0.5021011829376221\n","\n","Current batch: 200\n","Current train accuracy: 0.75\n","Current train loss: 0.5235405564308167\n","\n","Current batch: 300\n","Current train accuracy: 0.78125\n","Current train loss: 0.5389544367790222\n","\n","Current batch: 400\n","Current train accuracy: 0.8125\n","Current train loss: 0.49735066294670105\n","\n","Current batch: 500\n","Current train accuracy: 0.8125\n","Current train loss: 0.44473087787628174\n","\n","Current batch: 600\n","Current train accuracy: 0.75\n","Current train loss: 0.6292053461074829\n","\n","Current batch: 700\n","Current train accuracy: 0.875\n","Current train loss: 0.46309682726860046\n","\n","Current batch: 800\n","Current train accuracy: 0.75\n","Current train loss: 0.5089540481567383\n","\n","Current batch: 900\n","Current train accuracy: 0.84375\n","Current train loss: 0.5537542700767517\n","\n","Current batch: 1000\n","Current train accuracy: 0.6875\n","Current train loss: 0.7479853630065918\n","\n","Current batch: 1100\n","Current train accuracy: 0.8125\n","Current train loss: 0.5579619407653809\n","\n","Current batch: 1200\n","Current train accuracy: 0.84375\n","Current train loss: 0.530331015586853\n","\n","Current batch: 1300\n","Current train accuracy: 0.59375\n","Current train loss: 1.098850965499878\n","\n","Current batch: 1400\n","Current train accuracy: 0.78125\n","Current train loss: 0.8518771529197693\n","\n","Current batch: 1500\n","Current train accuracy: 0.71875\n","Current train loss: 0.8740391731262207\n","\n","Total train accuarcy: 81.198\n","Total train loss: 863.5620409548283\n","\n","[ Test epoch: 9 ]\n","\n","Test accuarcy: 78.88\n","Test average loss: 0.019313382272422314\n","Model Saved!\n","\n","[Train epoch: 10]\n","\n","Current batch: 100\n","Current train accuracy: 0.9375\n","Current train loss: 0.3872772455215454\n","\n","Current batch: 200\n","Current train accuracy: 0.84375\n","Current train loss: 0.6245443820953369\n","\n","Current batch: 300\n","Current train accuracy: 0.78125\n","Current train loss: 0.5234512090682983\n","\n","Current batch: 400\n","Current train accuracy: 0.8125\n","Current train loss: 0.5684757232666016\n","\n","Current batch: 500\n","Current train accuracy: 0.90625\n","Current train loss: 0.4420771300792694\n","\n","Current batch: 600\n","Current train accuracy: 0.8125\n","Current train loss: 0.7188844680786133\n","\n","Current batch: 700\n","Current train accuracy: 0.8125\n","Current train loss: 0.5910083055496216\n","\n","Current batch: 800\n","Current train accuracy: 0.78125\n","Current train loss: 0.7353058457374573\n","\n","Current batch: 900\n","Current train accuracy: 0.625\n","Current train loss: 1.019823431968689\n","\n","Current batch: 1000\n","Current train accuracy: 0.75\n","Current train loss: 0.5049194693565369\n","\n","Current batch: 1100\n","Current train accuracy: 0.90625\n","Current train loss: 0.2646932005882263\n","\n","Current batch: 1200\n","Current train accuracy: 0.8125\n","Current train loss: 0.624953031539917\n","\n","Current batch: 1300\n","Current train accuracy: 0.8125\n","Current train loss: 0.5928986668586731\n","\n","Current batch: 1400\n","Current train accuracy: 0.90625\n","Current train loss: 0.214735209941864\n","\n","Current batch: 1500\n","Current train accuracy: 0.84375\n","Current train loss: 0.4683178961277008\n","\n","Total train accuarcy: 81.648\n","Total train loss: 841.4127264022827\n","\n","[ Test epoch: 10 ]\n","\n","Test accuarcy: 77.77\n","Test average loss: 0.01958954669982195\n","Model Saved!\n","\n","[Train epoch: 11]\n","\n","Current batch: 100\n","Current train accuracy: 0.90625\n","Current train loss: 0.45507898926734924\n","\n","Current batch: 200\n","Current train accuracy: 0.8125\n","Current train loss: 0.48768380284309387\n","\n","Current batch: 300\n","Current train accuracy: 0.71875\n","Current train loss: 0.8367971777915955\n","\n","Current batch: 400\n","Current train accuracy: 0.8125\n","Current train loss: 0.43003973364830017\n","\n","Current batch: 500\n","Current train accuracy: 0.84375\n","Current train loss: 0.2895216941833496\n","\n","Current batch: 600\n","Current train accuracy: 0.84375\n","Current train loss: 0.4332706332206726\n","\n","Current batch: 700\n","Current train accuracy: 0.875\n","Current train loss: 0.419585645198822\n","\n","Current batch: 800\n","Current train accuracy: 0.84375\n","Current train loss: 0.37300801277160645\n","\n","Current batch: 900\n","Current train accuracy: 0.8125\n","Current train loss: 0.605341374874115\n","\n","Current batch: 1000\n","Current train accuracy: 0.8125\n","Current train loss: 0.5061556100845337\n","\n","Current batch: 1100\n","Current train accuracy: 0.75\n","Current train loss: 0.6809102892875671\n","\n","Current batch: 1200\n","Current train accuracy: 0.8125\n","Current train loss: 0.4381333589553833\n","\n","Current batch: 1300\n","Current train accuracy: 0.71875\n","Current train loss: 0.7668668627738953\n","\n","Current batch: 1400\n","Current train accuracy: 0.78125\n","Current train loss: 0.5092311501502991\n","\n","Current batch: 1500\n","Current train accuracy: 0.90625\n","Current train loss: 0.3621397018432617\n","\n","Total train accuarcy: 82.088\n","Total train loss: 820.2890218347311\n","\n","[ Test epoch: 11 ]\n","\n","Test accuarcy: 74.59\n","Test average loss: 0.024243598988652228\n","Model Saved!\n","\n","[Train epoch: 12]\n","\n","Current batch: 100\n","Current train accuracy: 0.75\n","Current train loss: 0.7474428415298462\n","\n","Current batch: 200\n","Current train accuracy: 0.875\n","Current train loss: 0.2992878258228302\n","\n","Current batch: 300\n","Current train accuracy: 0.84375\n","Current train loss: 0.39272525906562805\n","\n","Current batch: 400\n","Current train accuracy: 0.75\n","Current train loss: 0.5974041819572449\n","\n","Current batch: 500\n","Current train accuracy: 0.875\n","Current train loss: 0.2872418761253357\n","\n","Current batch: 600\n","Current train accuracy: 0.84375\n","Current train loss: 0.4334697723388672\n","\n","Current batch: 700\n","Current train accuracy: 0.75\n","Current train loss: 0.6359285712242126\n","\n","Current batch: 800\n","Current train accuracy: 0.90625\n","Current train loss: 0.4470904767513275\n","\n","Current batch: 900\n","Current train accuracy: 0.84375\n","Current train loss: 0.43735799193382263\n","\n","Current batch: 1000\n","Current train accuracy: 0.84375\n","Current train loss: 0.5131953358650208\n","\n","Current batch: 1100\n","Current train accuracy: 0.84375\n","Current train loss: 0.37274855375289917\n","\n","Current batch: 1200\n","Current train accuracy: 0.90625\n","Current train loss: 0.3115561306476593\n","\n","Current batch: 1300\n","Current train accuracy: 0.71875\n","Current train loss: 0.9808380007743835\n","\n","Current batch: 1400\n","Current train accuracy: 0.65625\n","Current train loss: 0.7835635542869568\n","\n","Current batch: 1500\n","Current train accuracy: 0.8125\n","Current train loss: 0.6607625484466553\n","\n","Total train accuarcy: 82.434\n","Total train loss: 805.604324862361\n","\n","[ Test epoch: 12 ]\n","\n","Test accuarcy: 81.14\n","Test average loss: 0.017769371300935744\n","Model Saved!\n","\n","[Train epoch: 13]\n","\n","Current batch: 100\n","Current train accuracy: 0.875\n","Current train loss: 0.4952775239944458\n","\n","Current batch: 200\n","Current train accuracy: 0.96875\n","Current train loss: 0.23247933387756348\n","\n","Current batch: 300\n","Current train accuracy: 0.8125\n","Current train loss: 0.5969986319541931\n","\n","Current batch: 400\n","Current train accuracy: 0.8125\n","Current train loss: 0.627522349357605\n","\n","Current batch: 500\n","Current train accuracy: 0.71875\n","Current train loss: 0.6178407669067383\n","\n","Current batch: 600\n","Current train accuracy: 0.90625\n","Current train loss: 0.3567432165145874\n","\n","Current batch: 700\n","Current train accuracy: 0.78125\n","Current train loss: 0.401777446269989\n","\n","Current batch: 800\n","Current train accuracy: 0.84375\n","Current train loss: 0.3873428404331207\n","\n","Current batch: 900\n","Current train accuracy: 0.875\n","Current train loss: 0.42168617248535156\n","\n","Current batch: 1000\n","Current train accuracy: 0.90625\n","Current train loss: 0.29945507645606995\n","\n","Current batch: 1100\n","Current train accuracy: 0.8125\n","Current train loss: 0.4323679208755493\n","\n","Current batch: 1200\n","Current train accuracy: 0.9375\n","Current train loss: 0.3539540767669678\n","\n","Current batch: 1300\n","Current train accuracy: 0.84375\n","Current train loss: 0.4685051739215851\n","\n","Current batch: 1400\n","Current train accuracy: 0.8125\n","Current train loss: 0.47925928235054016\n","\n","Current batch: 1500\n","Current train accuracy: 0.84375\n","Current train loss: 0.5215583443641663\n","\n","Total train accuarcy: 82.826\n","Total train loss: 792.0677865073085\n","\n","[ Test epoch: 13 ]\n","\n","Test accuarcy: 76.31\n","Test average loss: 0.024413284626603127\n","Model Saved!\n","\n","[Train epoch: 14]\n","\n","Current batch: 100\n","Current train accuracy: 0.875\n","Current train loss: 0.3419027626514435\n","\n","Current batch: 200\n","Current train accuracy: 0.78125\n","Current train loss: 0.6043853759765625\n","\n","Current batch: 300\n","Current train accuracy: 0.8125\n","Current train loss: 0.5247023105621338\n","\n","Current batch: 400\n","Current train accuracy: 0.90625\n","Current train loss: 0.3297785818576813\n","\n","Current batch: 500\n","Current train accuracy: 0.875\n","Current train loss: 0.43407711386680603\n","\n","Current batch: 600\n","Current train accuracy: 0.78125\n","Current train loss: 0.5022243857383728\n","\n","Current batch: 700\n","Current train accuracy: 0.78125\n","Current train loss: 0.39854326844215393\n","\n","Current batch: 800\n","Current train accuracy: 0.84375\n","Current train loss: 0.5866965055465698\n","\n","Current batch: 900\n","Current train accuracy: 0.875\n","Current train loss: 0.35698819160461426\n","\n","Current batch: 1000\n","Current train accuracy: 0.84375\n","Current train loss: 0.4779224693775177\n","\n","Current batch: 1100\n","Current train accuracy: 0.84375\n","Current train loss: 0.3410671055316925\n","\n","Current batch: 1200\n","Current train accuracy: 0.71875\n","Current train loss: 0.7258039116859436\n","\n","Current batch: 1300\n","Current train accuracy: 0.75\n","Current train loss: 0.6642272472381592\n","\n","Current batch: 1400\n","Current train accuracy: 0.78125\n","Current train loss: 0.6817716956138611\n","\n","Current batch: 1500\n","Current train accuracy: 0.8125\n","Current train loss: 0.4854278266429901\n","\n","Total train accuarcy: 83.254\n","Total train loss: 770.5676620975137\n","\n","[ Test epoch: 14 ]\n","\n","Test accuarcy: 83.7\n","Test average loss: 0.015440190920233727\n","Model Saved!\n","\n","[Train epoch: 15]\n","\n","Current batch: 100\n","Current train accuracy: 0.9375\n","Current train loss: 0.25283756852149963\n","\n","Current batch: 200\n","Current train accuracy: 0.90625\n","Current train loss: 0.3855547606945038\n","\n","Current batch: 300\n","Current train accuracy: 0.71875\n","Current train loss: 0.528886616230011\n","\n","Current batch: 400\n","Current train accuracy: 0.78125\n","Current train loss: 1.0162767171859741\n","\n","Current batch: 500\n","Current train accuracy: 0.90625\n","Current train loss: 0.3415576219558716\n","\n","Current batch: 600\n","Current train accuracy: 0.875\n","Current train loss: 0.3363684117794037\n","\n","Current batch: 700\n","Current train accuracy: 0.8125\n","Current train loss: 0.46543219685554504\n","\n","Current batch: 800\n","Current train accuracy: 0.75\n","Current train loss: 0.6038612127304077\n","\n","Current batch: 900\n","Current train accuracy: 0.75\n","Current train loss: 0.6145851612091064\n","\n","Current batch: 1000\n","Current train accuracy: 0.9375\n","Current train loss: 0.43170279264450073\n","\n","Current batch: 1100\n","Current train accuracy: 0.875\n","Current train loss: 0.4220455288887024\n","\n","Current batch: 1200\n","Current train accuracy: 0.8125\n","Current train loss: 0.49865826964378357\n","\n","Current batch: 1300\n","Current train accuracy: 0.8125\n","Current train loss: 0.5935977101325989\n","\n","Current batch: 1400\n","Current train accuracy: 0.75\n","Current train loss: 0.8056957125663757\n","\n","Current batch: 1500\n","Current train accuracy: 0.78125\n","Current train loss: 0.40379798412323\n","\n","Total train accuarcy: 83.29\n","Total train loss: 769.7659379169345\n","\n","[ Test epoch: 15 ]\n","\n","Test accuarcy: 80.81\n","Test average loss: 0.0185388828843832\n","Model Saved!\n","\n","[Train epoch: 16]\n","\n","Current batch: 100\n","Current train accuracy: 0.90625\n","Current train loss: 0.3454068601131439\n","\n","Current batch: 200\n","Current train accuracy: 0.6875\n","Current train loss: 0.6421130895614624\n","\n","Current batch: 300\n","Current train accuracy: 0.96875\n","Current train loss: 0.33155006170272827\n","\n","Current batch: 400\n","Current train accuracy: 0.90625\n","Current train loss: 0.3463694155216217\n","\n","Current batch: 500\n","Current train accuracy: 0.8125\n","Current train loss: 0.6964303851127625\n","\n","Current batch: 600\n","Current train accuracy: 0.71875\n","Current train loss: 0.6550453901290894\n","\n","Current batch: 700\n","Current train accuracy: 0.84375\n","Current train loss: 0.45924654603004456\n","\n","Current batch: 800\n","Current train accuracy: 0.78125\n","Current train loss: 0.6841589212417603\n","\n","Current batch: 900\n","Current train accuracy: 0.875\n","Current train loss: 0.41601359844207764\n","\n","Current batch: 1000\n","Current train accuracy: 0.8125\n","Current train loss: 0.7570877075195312\n","\n","Current batch: 1100\n","Current train accuracy: 0.625\n","Current train loss: 0.9356260299682617\n","\n","Current batch: 1200\n","Current train accuracy: 0.9375\n","Current train loss: 0.26255539059638977\n","\n","Current batch: 1300\n","Current train accuracy: 0.875\n","Current train loss: 0.3753533661365509\n","\n","Current batch: 1400\n","Current train accuracy: 0.875\n","Current train loss: 0.3978020250797272\n","\n","Current batch: 1500\n","Current train accuracy: 0.71875\n","Current train loss: 1.0424515008926392\n","\n","Total train accuarcy: 83.64\n","Total train loss: 754.6699114888906\n","\n","[ Test epoch: 16 ]\n","\n","Test accuarcy: 78.12\n","Test average loss: 0.021106263540685177\n","Model Saved!\n","\n","[Train epoch: 17]\n","\n","Current batch: 100\n","Current train accuracy: 0.8125\n","Current train loss: 0.636669933795929\n","\n","Current batch: 200\n","Current train accuracy: 0.84375\n","Current train loss: 0.49755987524986267\n","\n","Current batch: 300\n","Current train accuracy: 0.6875\n","Current train loss: 0.891789436340332\n","\n","Current batch: 400\n","Current train accuracy: 0.8125\n","Current train loss: 0.583086371421814\n","\n","Current batch: 500\n","Current train accuracy: 0.875\n","Current train loss: 0.4075280725955963\n","\n","Current batch: 600\n","Current train accuracy: 0.84375\n","Current train loss: 0.40052974224090576\n","\n","Current batch: 700\n","Current train accuracy: 0.8125\n","Current train loss: 0.7334440350532532\n","\n","Current batch: 800\n","Current train accuracy: 0.9375\n","Current train loss: 0.2085830271244049\n","\n","Current batch: 900\n","Current train accuracy: 0.875\n","Current train loss: 0.4610488712787628\n","\n","Current batch: 1000\n","Current train accuracy: 0.84375\n","Current train loss: 0.531527578830719\n","\n","Current batch: 1100\n","Current train accuracy: 0.8125\n","Current train loss: 0.7262763381004333\n","\n","Current batch: 1200\n","Current train accuracy: 0.8125\n","Current train loss: 0.5597930550575256\n","\n","Current batch: 1300\n","Current train accuracy: 0.875\n","Current train loss: 0.3377574682235718\n","\n","Current batch: 1400\n","Current train accuracy: 0.84375\n","Current train loss: 0.4754404127597809\n","\n","Current batch: 1500\n","Current train accuracy: 0.875\n","Current train loss: 0.5152708292007446\n","\n","Total train accuarcy: 83.896\n","Total train loss: 743.3470407873392\n","\n","[ Test epoch: 17 ]\n","\n","Test accuarcy: 80.85\n","Test average loss: 0.01855920171663165\n","Model Saved!\n","\n","[Train epoch: 18]\n","\n","Current batch: 100\n","Current train accuracy: 0.84375\n","Current train loss: 0.4161202013492584\n","\n","Current batch: 200\n","Current train accuracy: 0.9375\n","Current train loss: 0.31760483980178833\n","\n","Current batch: 300\n","Current train accuracy: 0.75\n","Current train loss: 0.7762935161590576\n","\n","Current batch: 400\n","Current train accuracy: 0.84375\n","Current train loss: 0.6160149574279785\n","\n","Current batch: 500\n","Current train accuracy: 0.75\n","Current train loss: 0.46611106395721436\n","\n","Current batch: 600\n","Current train accuracy: 0.8125\n","Current train loss: 0.3977885842323303\n","\n","Current batch: 700\n","Current train accuracy: 0.78125\n","Current train loss: 0.5580390691757202\n","\n","Current batch: 800\n","Current train accuracy: 0.84375\n","Current train loss: 0.5426462888717651\n","\n","Current batch: 900\n","Current train accuracy: 0.75\n","Current train loss: 0.7278072834014893\n","\n","Current batch: 1000\n","Current train accuracy: 0.6875\n","Current train loss: 0.7127862572669983\n","\n","Current batch: 1100\n","Current train accuracy: 0.9375\n","Current train loss: 0.24820609390735626\n","\n","Current batch: 1200\n","Current train accuracy: 0.875\n","Current train loss: 0.3564223647117615\n","\n","Current batch: 1300\n","Current train accuracy: 0.78125\n","Current train loss: 0.7271227836608887\n","\n","Current batch: 1400\n","Current train accuracy: 0.9375\n","Current train loss: 0.2509148418903351\n","\n","Current batch: 1500\n","Current train accuracy: 0.78125\n","Current train loss: 0.6819278597831726\n","\n","Total train accuarcy: 83.748\n","Total train loss: 738.607757627964\n","\n","[ Test epoch: 18 ]\n","\n","Test accuarcy: 80.33\n","Test average loss: 0.01837267823368311\n","Model Saved!\n","\n","[Train epoch: 19]\n","\n","Current batch: 100\n","Current train accuracy: 0.84375\n","Current train loss: 0.41993430256843567\n","\n","Current batch: 200\n","Current train accuracy: 1.0\n","Current train loss: 0.18410897254943848\n","\n","Current batch: 300\n","Current train accuracy: 0.84375\n","Current train loss: 0.3489091694355011\n","\n","Current batch: 400\n","Current train accuracy: 0.90625\n","Current train loss: 0.34090346097946167\n","\n","Current batch: 500\n","Current train accuracy: 0.84375\n","Current train loss: 0.5022991895675659\n","\n","Current batch: 600\n","Current train accuracy: 0.875\n","Current train loss: 0.4431684613227844\n","\n","Current batch: 700\n","Current train accuracy: 0.8125\n","Current train loss: 0.577267050743103\n","\n","Current batch: 800\n","Current train accuracy: 0.90625\n","Current train loss: 0.35375073552131653\n","\n","Current batch: 900\n","Current train accuracy: 0.84375\n","Current train loss: 0.43225938081741333\n","\n","Current batch: 1000\n","Current train accuracy: 0.90625\n","Current train loss: 0.387813538312912\n","\n","Current batch: 1100\n","Current train accuracy: 0.84375\n","Current train loss: 0.6378630995750427\n","\n","Current batch: 1200\n","Current train accuracy: 0.71875\n","Current train loss: 0.5461245179176331\n","\n","Current batch: 1300\n","Current train accuracy: 0.8125\n","Current train loss: 0.5531685948371887\n","\n","Current batch: 1400\n","Current train accuracy: 0.84375\n","Current train loss: 0.5663416981697083\n","\n","Current batch: 1500\n","Current train accuracy: 0.8125\n","Current train loss: 0.6029592752456665\n","\n","Total train accuarcy: 84.076\n","Total train loss: 735.165260873735\n","\n","[ Test epoch: 19 ]\n","\n","Test accuarcy: 82.69\n","Test average loss: 0.015954626639187335\n","Model Saved!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wF3VPpCLzCkA","executionInfo":{"status":"ok","timestamp":1688725276364,"user_tz":-540,"elapsed":34,"user":{"displayName":"바다","userId":"15013850930390190862"}}},"execution_count":28,"outputs":[]}]}